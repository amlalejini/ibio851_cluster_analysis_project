---
title: "Cluster Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Authors: Miranda Wade, Matthew Andres Moreno, Alexander Lalejini, and Nate Davis

TODOs:

 * Linkify shit.
 * Add in more citations. 
 * In-text citations + references.
 * Add in link to clustering demo

##A (Brief) Introduction to Cluster Analysis
Cluster analysis is a type of machine learning that attempts to divide or organize data into groups (*i.e.* clusters) such that data objects (*i.e.* data points, observations, *etc.*) within a particular cluster are more similar to one another than to data objects in other clusters [CITE: Tan]. Cluster analysis is often used to automatically categorize unlabeled data, to infer relationships within data, or to summarize data, reducing a large data set down to a few representative clusters [CITE: Tan]. 

<TODO: Maybe examples here> 

There are many cluster analysis techniques. Different approaches provide different ways of grouping data together, and different cluster analysis technqies may be more or less appropriate for grouping certain types of data. Below, we explain and demonstrate (in R) three different cluster analysis techniques: prototype-based clustering, hierarchical clustering, and density-based clustering. 

##Cluster Analysis Techniques
In this section, we explore prototype-based clustering, hierarchical clustering, and density-based clustering. We  provide an overview of each technique along with a canned (*i.e.* already existing) R function/package capable of doing all of the heavy cluster analysis for you right out of the box! Additionally, we'll take a peek under the hood of the K-means prototype-based clustering algorithm by stepping through a from-scratch R implementation. 

###Prototype-based Clustering
####Overview
In prototype-based clustering, a cluster is defined by a representative *prototype* object where each member of a cluster is more similar to that cluster's prototype than to any other cluster's prototype [CITE]. But, what exactly is a prototype object? 

If the data being clustered is numeric, the prototype is often the centroid (i.e. the center or average) of all of the cluster members [CITE: kumar dm textbook]. For example, if we had the following two clusters of numeric data points:

```{r}
cluster_1 <- c(10, 7, 10, 11, 9)
cluster_2 <- c(23, 22, 19, 22, 21)
```

The cluster prototypes would be their centroids:
```{r}
cluster_1.prototype <- mean(cluster_1)
cluster_2.prototype <- mean(cluster_2)
```
 * Cluster 1 prototype = `r cluster_1.prototype`
 * Cluster 2 prototype = `r cluster_2.prototype`

If the data is not numeric, the appropriate kind of prototype will depend on the data you are clustering. The prototype should be as representative as possible of the cluster members, and you should be able to calculate distance between any given cluster prototype and any given data point.

For example, if we were clustering a list of three letter words together based on their spelling, one of our clusters might look like:
```{r}
word_cluster <- c("dog", "doc", "dot", "bog", "log")
```

We might say the prototype of that cluster is a three letter word (not necessarily a *real* word that shows up in our list) where each character position in the prototype is the most abundant character at that position among all of the cluster's member words. With this definition of a prototype, `"dog"` would be our example cluster's prototype. In case you're wondering how one might compute distances between sequences of characters (useful for clustering DNA sequences), you could use the [edit distance](https://en.wikipedia.org/wiki/Edit_distance) metric. 

####Requirements
 * You must be able to define a prototype given an arbitrary grouping of the data you are clustering.
 * You must be able to compute a measure of either similarity or dissimilarity between prototypes and data points. 

####Coding From Scratch
Here, we demonstrate a basic implementation of the K-means algorithm, which is one of the most commonly used algorithms for prototype-based clustering. It makes for a nice introduction to prototype-based clustering as it's fairly simple and intuitive. 

**Disclaimer**: Our from-scratch implementation is meant as a demonstrative example. Thus, our code is implemented to maximize understandability *not* necessarily efficiency. 

The K-means clustering algorithm makes several assumptions that we'll need to be aware of:

 * *A priori* knowledge of the number of clusters in your data. Note: you can relax this assumption by making use of a variety of techniques that estimate the most appropriate number clusters in your data. We will not cover these techniques here, but more information can be found [TODO: CITE]. 
 * You are capable of computing the mean of a grouping of data. 
 * You are capable of computing the distance between a data point and a prototype.
 * Clusters should be relatively globular (*e.g.* normally distributed around a central point). 

For our demonstration, we'll generate some 2-dimentional numeric data:
```{r}
c.size <- 100
c1.true <- c(16, 128)
c2.true <- c(128, 16)
c3.true <- c(128, 128)
c1.gen <- cbind(rnorm(c.size, mean=c1.true[1], sd=8), rnorm(c.size, mean=c1.true[2], sd=8))
c2.gen <- cbind(rnorm(c.size, mean=c2.true[1], sd=8), rnorm(c.size, mean=c2.true[2], sd=8))
c3.gen <- cbind(rnorm(c.size, mean=c3.true[1], sd=8), rnorm(c.size, mean=c3.true[2], sd=8))

kmeans.data <- rbind(c1.gen, c2.gen, c3.gen)
plot(kmeans.data)
```

Okay, so we have some data to cluster. Let's get started with our K-means implementation. First, we need to do a little setup: we need to define our distance function, and we need to define the parameter k -- the number of clusters to find.

```{r}
euclidean.distance.2d <- function(a, b) {  return(sqrt((a[1] - b[1])**2 + (a[2] - b[2])**2)) }

k <- 3
```

Step 1: Randomly assign each data point to a cluster.
```{r}
# To do this, we can shuffle our data around then assign the first third of the data to cluster 1, the second third 
# to cluster 2, and the last third of the data to cluster 3. 
kmeans.data <- kmeans.data[sample(nrow(kmeans.data)),] 
cluster.ids <- rep(1:3, rep(nrow(kmeans.data)/k, k))
cluster.centroids <- rep(0, k)
```
Let's take a look at how our randomly initialized clusters look (where colors indicate cluster membership): 
```{r echo=FALSE}
plot(kmeans.data, col=cluster.ids)
```

We'll the following steps until the algorithm converges: update cluster centroids, then reassign data points to the nearest cluster centroid. In this way, we'll iteratively refine our clusters. 

```{r}
while (TRUE) {
  # Step 2: Update cluster centroids.
  c.sizes <- rep(0, k)                   # Used to keep track of cluster sizes.
  c.totals <- matrix(0, nrow=k, ncol=2)  # Used to keep track of cluster magnitudes (intermediate step in computing centroids)
  for (j in 1:length(cluster.ids)) {
    c.id <- cluster.ids[j]
    c.sizes[c.id] <- c.sizes[c.id] + 1
    c.totals[c.id,1] <- c.totals[c.id,1] + kmeans.data[j,1]
    c.totals[c.id,2] <- c.totals[c.id,2] + kmeans.data[j,2]
  }
  # Compute centroids.
  cluster.centroids <- c.totals / c.sizes
  
  # Step 3: Reassign data points to cluster with closest centroid.
  # If nothing changes, we'll say that clustering has converged.
  something_changed <- FALSE
  for (j in 1:length(cluster.ids)) {
    min_dist <- Inf
    c.id <- 0
    for (c in 1:k) {
      dist <- euclidean.distance.2d(kmeans.data[j,], cluster.centroids[c,])
      if (dist < min_dist) {
        min_dist <- dist
        c.id <- c
      }
    }
    if (c.id != cluster.ids[j]) { something_changed <- TRUE }
    cluster.ids[j] <- c.id
  }
  if (!something_changed) { break }
}
```

Let's take a look at the results of our clustering (where colors indicate cluster membership):
```{r echo=FALSE}
plot(kmeans.data, col=cluster.ids)
```

####Coding From the Can
R has a built-in K-means clustering function called `kmeans`. To cluster our generated data with the built-in K-means function, we would:
```{r}
kmeans.cluster <- kmeans(kmeans.data, k, 1000)
plot(kmeans.data, col=kmeans.cluster[["cluster"]])
```

###Hierarchical Clustering
####Overview
The hierarchical clustering analysis produces hierarchical groupings of data. For example, hierarchical cluster analysis can be used to infer phylogenetic relationships among species, as a phylogenetic tree is simply a hirarchical grouping of species where the groupings are based on genetic relatedness [CITE EXAMPLE]. 

There are two types of hierarchical clustering approaches: divisive and agglomerative. 

**Divisive hierarchical clustering** is a top down approach where all observations begin as a single cluster, and clusters are iteratively divided until each cluster has a size of one [CITE]. 

**Agglomerative hierarchical clustering** is a bottom up approach where each observation (*i.e.* data point) is initially a single cluster, and clusters are iteratively merged until there is only a single cluster remaining that contains all of the data [CITE]. Agglomerative hierarchical clustering is typically computationally less expensive than divisive hierarchical clustering; thus, it is used more often. For example, the built-in hierarchical clustering function in R implements agglomerative hierarchical clustering. 

####Requirements
 * Agglomerative hierarchical clustering
     * Requires a mechanism for merging two clusters together, which often requires a way to compute a similarity or dissimilarity score between two clusters of arbitrary size.
 * Divisive hierarchical clustering
     * Requires a mechanism for dividing a single cluster of size greater than one into two or more smaller clusters. 

####Coding From the Can
R has a built-in (agglomerative) hierarchical clustering function called `hclust`.

Let's generate some data to cluster hierarchically. To keep things simple, we'll generate some 1-dimensional numeric data.
```{r}
# This should give us three normally distributed clusters where each cluster's mean is uniformally distributed between 1 and 100 with a sd of 10.
hclust.data <- floor(rnorm(10, floor(runif(3, 1, 100)), 10))
```

Unlike the built-in `kmeans` function, the built-in `hclust` function does not take your data as input directly. Rather, it requires a dissimilarity structure, which is just a matrix of pairwise dissimilarity scores between all of your observations/data points. R has a nice built-in function to do this for us: `dist`. 
```{r}
# We'll provide our data and that we'd like to use euclidean distance.
dissim.mat <- dist(hclust.data, method="euclidean")
```

Now that we have our dissimilarity structure, we can plug that right into the `hclust` function.
```{r}
hclusters <- hclust(dissim.mat)
```

We can check out our results in dendrogram form:
```{r}
plot(hclusters, labels=as.character(hclust.data))
```

I meantioned that hierarical clustering used to infer relationships between character sequences such as DNA sequences. Our above example relies on the `dist` function, which operates on numeric data, to produce our dissimilarity matrix. What if our data is a list of character sequences?
```{r}
hclust.data.char <- c("dog", "doc", "log", "blog", "cat", "category", "statistics")
```

Instead of using the `dist` function to compute the pairwise euclidean distance between all of our words, we can use the `adist` function to compute a dissimilarity matrix for character sequeces where each pairwise dissimilarity score between two sequences is the [edit (also known as Levenshtein) distance](https://en.wikipedia.org/wiki/Edit_distance) between the two sequences.

```{r}
dissim.mat.char <- adist(hclust.data.char)
```

Now let's do some clustering and check out the results!
```{r}
hclusters.char <- hclust(as.dist(dissim.mat.char))
plot(hclusters.char, labels=hclust.data.char)
```

###Density-based Clustering

####Overview
In density-based clustering, a cluster is defined as a high density region of data that is surrounded by a low density region of data [4]. While density might be a strange concept to apply to many types of data outside of spatial data, it really is quite intuitive and is best demonstrated visually.

Density-based clustering deals well with irregular and non-globular data (unlike the other approaches we’ve discussed so far) and is less impacted by noise and outliers [4].

In this presentation of density-based clustering, we’ll focus on a particular representative density-based clustering algorithm called DBSCAN (stands for density-based spatial clustering of applications with noise)  {cite Ester et. al, 1996}. Other density-based clustering algorithms that follow the same general idea (but with important differences!) exist, notably OPTICS, DENCLUE, STING, and CLIQUE.

#### DBSCAN Approach

DBSCAN works by sorting data points into three categories:
1. core points,
2. border points, and
3. outliers.

Core points, intuitively, are the points in the middle of a dense cluster. Because they’re in the middle of a dense cluster they have lots of other points nearby. Border points, intuitively, are the points at the edge of a dense cluster. Because they’re on the edge, they don’t have as many other points nearby. However, they are nearby to at least one core point. Finally, outliers, aren’t close to lots of other points like a core point or close to a core point like a border point.

This image shows an example of outlier, border, and core points in context.

![](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_07_unsupervised_learning/4.2-lesson/assets/images/dbscan.png)

{cite https://bradzzz.gitbooks.io/ga-seattle-dsi/content/}

In order to sort data points into these three categories formally, DBSCAN requires two parameters to be defined:
* \epsilon: the radius of a neighborhood around a data point (this defines our idea of “nearby”)
* minPts: the minimum number of data points (this defines our idea of “a lot”)

Formally,
1. a point p is a core point if there are at least minPts other data points within \epsilon of p.
2. a point p is a border point if it is not a core point but is within \epsilon a core point,
3. a point p is an outlier if it is neither a core point nor a border point.

Once we have all of our data points labeled as core, border, or outlier, we can construct clusters. This is performed in a very straightforward manner. Essentially, core points that can reach each other by any number of jumps between core points of hop length \epsilon or less are labeled as belonging to a particular cluster. Then, any border points are labeled as belonging to the same cluster as the core point in their \epsilon neighborhood. {cite DBSCAN DOMINO}

#### DBSCAN Advantages {cite Wikipedia}
* No need to specify number of clusters
* Flexible to cluster shape (can recognize lots of different cluster shapes)
* Robust to outliers and noise
* Just two parameters, both of which have intuitive interpretations

#### DBSCAN Disadvantages {cite Wikipedia}
* Depends on distance metric between points, which may become less effective for high-dimensional data
* Outcome highly sensitive to parameter settings
* Stumbles when clustering datasets where clusters have large differences in density

#### Coding From the Can
The DBSCAN R package contains a convenient implementation of the DBSCAN density-based cluster analysis algorithm.

##Assessing Cluster Quality
The quality of a clustering decision can be assessed in two main ways:

 1. **Internal Evaluation**: assess the extent to which data points within *the same* predicted cluster are similar to one another, and assess how different data points within *different* predicted clusters are to one another.  
 2. **External Evaluation**: assess the ability of clustering results to predict a set of known classes underlying the data. 
 
It is important to note that internal evaluation does not measure the ability of a clustering algorithm to cluster data in a way that reflects the ground truth classification scheme (*i.e.* the ability to create *valid* clusters). Instead, it measures the ability of a clustering algorithm to create, essentially, nicely clumped clusters that are clearly resolved from one another. 

Similarly, it should be noted that external evaluation assumes some known underlying class scheme that we expect to be reflected in the observed data clusters (*i.e.* data points from the same explanatory class tend to cluster together). The set of known classifications might come from data labels (*e.g.* if you're performing genetic clustering, this might be a set of distinct locations organisms were sampled from) or might be human annotated post-hoc based on what an expert manually determines to be valid clusters. Without these class labels, though, external evaluation cannot be performed. 

Thus, if you're working with unlabeled data, internal evaluation is the way to go. This is especially useful if you're tuning the parameters of your clustering algorithm on unlabeled daata and trying to determine how many clusters exist in the first place. You might use internal evaluation to evaluate how neatly known data labels clump together, too. if you want to assess the ability of the clustering process to reflect a certain ground truth, use an external evaluation scheme. 

For both internal and external evaluation, several different metrics exist. In order to actually perform internal or external evaluation, you will need to select a particular metric to work with (or try several). [Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis#Evaluation_and_assessment) is a great starting point for this. 

##References